{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75eca928-5da5-489d-83ef-6a72ea6aea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 15:18:40.311946: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification,BertConfig\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a81a6-39da-44d9-b82e-f011f8e43fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea01e80c-48b8-4ec0-ad23-07a52fd6af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('prajjwal1/bert-mini')\n",
    "model = TFBertForSequenceClassification(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8700dd-139b-4a37-a8e1-7663a28ff139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ed1e85-cae8-4f8c-a5e8-2e83a18a3215",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'weights-bert.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights-bert.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'weights-bert.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights-bert.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08f62f-2d45-4667-afb4-20f75dfaab21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09d039c-c815-4346-8332-68ab7b88f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, threshold=0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "# Apply the threshold to the predicted values\n",
    "    y_pred = tf.cast(y_pred > threshold, dtype=tf.float32)\n",
    "\n",
    " # Calculate the Hamming Loss\\\n",
    "    epsilon = 1e-7  # Small epsilon value\\n\",\n",
    "    hamming_loss = 1.0 - tf.reduce_mean(tf.reduce_sum(y_true * y_pred, axis=1) / (tf.reduce_sum(y_true + y_pred - y_true * y_pred, axis=1) + epsilon))\n",
    "\n",
    "    return hamming_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6856e753-936a-41e8-b855-75540e9a55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1958700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 32)          4832      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, None, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2017338 (7.70 MB)\n",
      "Trainable params: 2017338 (7.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "232/232 [==============================] - 24s 93ms/step - loss: 0.5247 - accuracy: 0.3370 - val_loss: 0.5177 - val_accuracy: 0.3489\n",
      "Epoch 2/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.5162 - accuracy: 0.3500 - val_loss: 0.5168 - val_accuracy: 0.3488\n",
      "Epoch 3/150\n",
      "232/232 [==============================] - 21s 89ms/step - loss: 0.5156 - accuracy: 0.3494 - val_loss: 0.5164 - val_accuracy: 0.3491\n",
      "Epoch 4/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.5097 - accuracy: 0.3365 - val_loss: 0.4974 - val_accuracy: 0.3594\n",
      "Epoch 5/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.4595 - accuracy: 0.4237 - val_loss: 0.4296 - val_accuracy: 0.4974\n",
      "Epoch 6/150\n",
      "232/232 [==============================] - 20s 88ms/step - loss: 0.4032 - accuracy: 0.5326 - val_loss: 0.4003 - val_accuracy: 0.5452\n",
      "Epoch 7/150\n",
      "232/232 [==============================] - 20s 88ms/step - loss: 0.3664 - accuracy: 0.5978 - val_loss: 0.3835 - val_accuracy: 0.5793\n",
      "Epoch 8/150\n",
      "232/232 [==============================] - 20s 88ms/step - loss: 0.3797 - accuracy: 0.5606 - val_loss: 0.3835 - val_accuracy: 0.5726\n",
      "Epoch 9/150\n",
      "232/232 [==============================] - 21s 88ms/step - loss: 0.3346 - accuracy: 0.6206 - val_loss: 0.3642 - val_accuracy: 0.5722\n",
      "Epoch 10/150\n",
      "232/232 [==============================] - 21s 89ms/step - loss: 0.3088 - accuracy: 0.6466 - val_loss: 0.3572 - val_accuracy: 0.5882\n",
      "Epoch 11/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.2888 - accuracy: 0.6752 - val_loss: 0.3600 - val_accuracy: 0.5816\n",
      "Epoch 12/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.2735 - accuracy: 0.6937 - val_loss: 0.3720 - val_accuracy: 0.5814\n",
      "Epoch 13/150\n",
      "232/232 [==============================] - 21s 91ms/step - loss: 0.2601 - accuracy: 0.7088 - val_loss: 0.3754 - val_accuracy: 0.5860\n",
      "Epoch 14/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.2497 - accuracy: 0.7141 - val_loss: 0.3812 - val_accuracy: 0.5795\n",
      "442/442 [==============================] - 3s 8ms/step - loss: 0.3543 - accuracy: 0.5896\n",
      "The accuracy evaluated on the test set is of 58.957%\n",
      "Testing loss \t 35.43282151222229\n",
      "Testing accuracy  58.956754207611084\n"
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=embedding_size))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Students will be ending their code here\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Change the number of epochs and the batch size depending on the RAM Size\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    callbacks=[es], validation_split=0.3\n",
    ")\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d68c130-0d8f-43e8-8c4f-7b8e4054e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 4s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45775944"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e630f225-f3d7-40e8-9052-4a1599891cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 50)          1958700   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, None, 20)          4320      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 20)                2520      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1965666 (7.50 MB)\n",
      "Trainable params: 1965666 (7.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "232/232 [==============================] - 31s 101ms/step - loss: 0.5314 - accuracy: 0.3501 - val_loss: 0.5129 - val_accuracy: 0.3574\n",
      "Epoch 2/150\n",
      "232/232 [==============================] - 22s 93ms/step - loss: 0.4850 - accuracy: 0.4005 - val_loss: 0.4467 - val_accuracy: 0.4567\n",
      "Epoch 3/150\n",
      "232/232 [==============================] - 21s 89ms/step - loss: 0.4153 - accuracy: 0.4927 - val_loss: 0.3973 - val_accuracy: 0.4886\n",
      "Epoch 4/150\n",
      "232/232 [==============================] - 21s 91ms/step - loss: 0.3735 - accuracy: 0.5566 - val_loss: 0.3816 - val_accuracy: 0.5396\n",
      "Epoch 5/150\n",
      "232/232 [==============================] - 21s 92ms/step - loss: 0.3533 - accuracy: 0.5945 - val_loss: 0.3764 - val_accuracy: 0.5683\n",
      "Epoch 6/150\n",
      "232/232 [==============================] - 21s 89ms/step - loss: 0.3385 - accuracy: 0.6120 - val_loss: 0.3668 - val_accuracy: 0.5686\n",
      "Epoch 7/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.3257 - accuracy: 0.6261 - val_loss: 0.3609 - val_accuracy: 0.5679\n",
      "Epoch 8/150\n",
      "232/232 [==============================] - 20s 88ms/step - loss: 0.3147 - accuracy: 0.6400 - val_loss: 0.3594 - val_accuracy: 0.5731\n",
      "Epoch 9/150\n",
      "232/232 [==============================] - 20s 85ms/step - loss: 0.3045 - accuracy: 0.6486 - val_loss: 0.3590 - val_accuracy: 0.5725\n",
      "Epoch 10/150\n",
      "232/232 [==============================] - 20s 85ms/step - loss: 0.2954 - accuracy: 0.6628 - val_loss: 0.3593 - val_accuracy: 0.5749\n",
      "Epoch 11/150\n",
      "232/232 [==============================] - 20s 86ms/step - loss: 0.2877 - accuracy: 0.6738 - val_loss: 0.3595 - val_accuracy: 0.5743\n",
      "Epoch 12/150\n",
      "232/232 [==============================] - 21s 88ms/step - loss: 0.2798 - accuracy: 0.6821 - val_loss: 0.3590 - val_accuracy: 0.5825\n",
      "Epoch 13/150\n",
      "232/232 [==============================] - 20s 87ms/step - loss: 0.2741 - accuracy: 0.6891 - val_loss: 0.3629 - val_accuracy: 0.5883\n",
      "442/442 [==============================] - 6s 13ms/step - loss: 0.3559 - accuracy: 0.5794\n",
      "The accuracy evaluated on the test set is of 57.938%\n",
      "Testing loss \t 35.58516502380371\n",
      "Testing accuracy  57.93757438659668\n"
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(6, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=150,\n",
    "        batch_size=128,\n",
    "        callbacks=[es], validation_split=0.3\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b415ee06-6515-4d9e-97fd-378903cea860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 8s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46234578"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0272686-7c82-4b74-91bb-f0cbc65a2b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
