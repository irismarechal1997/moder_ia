{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75eca928-5da5-489d-83ef-6a72ea6aea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertConfig, AutoTokenizer, TFBertModel, BertTokenizer, TFBertForSequenceClassification, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "887fb641-a947-4808-a426-3f92c41937f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/irismarechal/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=6)\n",
    "model.build()\n",
    "model.load_weights(\"../data/bert_classif_VM.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2ff0ae9-f696-4a2e-9bd5-42184e529143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, threshold=0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "# Apply the threshold to the predicted values\n",
    "    y_pred = tf.cast(y_pred > threshold, dtype=tf.float32)\n",
    "\n",
    " # Calculate the Hamming Loss\\\n",
    "    epsilon = 1e-7  # Small epsilon value\\n\",\n",
    "    hamming_loss = 1.0 - tf.reduce_mean(tf.reduce_sum(y_true * y_pred, axis=1) / (tf.reduce_sum(y_true + y_pred - y_true * y_pred, axis=1) + epsilon))\n",
    "\n",
    "    return hamming_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d3eb97-6594-4939-af1d-63303c3e5531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 214s 7s/step\n"
     ]
    }
   ],
   "source": [
    "data_extract = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "data_extracted=data_extract[0:1000]\n",
    "\n",
    "#process X et Y\n",
    "texts = data_extracted['text_processed']\n",
    "labels = data_extracted.drop(['text','text_processed'],axis=1)\n",
    "\n",
    "#transfor the X into a list of string\n",
    "text_to_encode=texts.values.tolist()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_tokenized=tokenizer(text_to_encode, max_length=200, padding='max_length', return_attention_mask=True, return_tensors='tf')\n",
    "\n",
    "X_tuple = (X_tokenized['input_ids'], X_tokenized['token_type_ids'], X_tokenized['attention_mask'])\n",
    "\n",
    "\n",
    "#Y pred + apply function\n",
    "y_pred = model.predict(X_tuple)\n",
    "y_true = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d40e854e-9015-4f7f-81ae-91067f2f1bad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (TFSequenceClassifierOutput(loss=None, logits=array([[-0.64942425, -0.35383356, -0.50275403, -0.11968189, -0.2865996 ,\n        -0.42911953],\n       [-0.6881017 , -0.53711444, -0.5405972 , -0.23513767,  0.18102808,\n        -0.47056854],\n       [-0.37870422, -0.33187675, -0.40674347, -0.25147885, -0.39447534,\n        -0.5836401 ],\n       ...,\n       [-0.43493688, -0.20578143, -0.32088625, -0.3356344 , -0.43986544,\n        -0.44403508],\n       [-0.56450665, -0.29238895, -0.43692702, -0.1734829 , -0.4659426 ,\n        -0.5295357 ],\n       [-0.9298853 , -0.6529261 , -0.6143728 ,  0.38198948,  0.36555964,\n        -0.24915746]], dtype=float32), hidden_states=None, attentions=None)) with an unsupported type (<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[43mhamming_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m h3\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m, in \u001b[0;36mhamming_loss\u001b[0;34m(y_true, y_pred, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhamming_loss\u001b[39m(y_true, y_pred, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Apply the threshold to the predicted values\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_pred \u001b[38;5;241m>\u001b[39m threshold, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (TFSequenceClassifierOutput(loss=None, logits=array([[-0.64942425, -0.35383356, -0.50275403, -0.11968189, -0.2865996 ,\n        -0.42911953],\n       [-0.6881017 , -0.53711444, -0.5405972 , -0.23513767,  0.18102808,\n        -0.47056854],\n       [-0.37870422, -0.33187675, -0.40674347, -0.25147885, -0.39447534,\n        -0.5836401 ],\n       ...,\n       [-0.43493688, -0.20578143, -0.32088625, -0.3356344 , -0.43986544,\n        -0.44403508],\n       [-0.56450665, -0.29238895, -0.43692702, -0.1734829 , -0.4659426 ,\n        -0.5295357 ],\n       [-0.9298853 , -0.6529261 , -0.6143728 ,  0.38198948,  0.36555964,\n        -0.24915746]], dtype=float32), hidden_states=None, attentions=None)) with an unsupported type (<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06f807a4-2f70-4338-a00c-88b2db840aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = y_pred.logits\n",
    "probabilities = tf.nn.softmax(logits, axis=1)\n",
    "predicted_classes = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "# Create a dictionary containing the data\n",
    "data = {\n",
    "    \"Text\": texts,\n",
    "    \"Predicted_Class\": predicted_classes,\n",
    "}\n",
    "\n",
    "# Add probability columns for each class\n",
    "num_classes = model.config.num_labels\n",
    "for class_idx in range(num_classes):\n",
    "    data[f\"Probability_Class_{class_idx}\"] = probabilities[:, class_idx]\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c795185-1dbc-4386-bbf3-cc21551cdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Probability_Class_0', 'Probability_Class_1',\n",
    "       'Probability_Class_2', 'Probability_Class_3', 'Probability_Class_4',\n",
    "       'Probability_Class_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40c99406-841c-4f72-b704-4ed7626e226d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070498"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3 = hamming_loss(df, y_true)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38ddb9-b832-4553-b83d-329cb5ece4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df.to_numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381c183-567d-43b8-ac40-860876d26ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbedad-8d96-415f-81a9-4c10df703f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28f54b-54da-4c9f-a555-b2f8cdbfc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df=pd.DataFrame(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549ae43-9092-498c-8180-ea9e2561e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd0c42-1bc0-4dcc-bbf2-3f62f3f26b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = hamming_loss(y_true, y_pred_df)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dd18a2a-17a5-4aef-86a1-1d951d3b8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_1=\"black people should die\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "820ba082-8659-41f0-8bfb-b0fa8d3f43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tweet_tokenized=tokenizer(tweet_1, max_length=200, padding='max_length', return_attention_mask=True, return_tensors='tf')\n",
    "tweet_tuple = (tweet_tokenized['input_ids'], tweet_tokenized['token_type_ids'], tweet_tokenized['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcc7e8eb-e1fa-4761-a83f-2989ca34a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 503ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.9068673 , -0.6819405 , -0.66433483,  0.42534655,  0.62063384,\n",
       "        -0.19836582]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tweet = model.predict(tweet_tuple)\n",
    "y_pred_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fde0de32-83b2-42e8-aec4-d2ec680fad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07166547, 0.08974171, 0.09133568, 0.27157074, 0.33013776,\n",
       "        0.14554864]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = tf.nn.softmax(y_pred_tweet.logits, axis=-1).numpy()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48eea5a3-87fd-47dd-ac30-f9688085b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = tf.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34a0fcdf-4e3f-49c4-9e2e-42d7f7c10c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Predicted_Class</th>\n",
       "      <th>Probability_Class_0</th>\n",
       "      <th>Probability_Class_1</th>\n",
       "      <th>Probability_Class_2</th>\n",
       "      <th>Probability_Class_3</th>\n",
       "      <th>Probability_Class_4</th>\n",
       "      <th>Probability_Class_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>black people should die</td>\n",
       "      <td>4</td>\n",
       "      <td>0.071665</td>\n",
       "      <td>0.089742</td>\n",
       "      <td>0.091336</td>\n",
       "      <td>0.271571</td>\n",
       "      <td>0.330138</td>\n",
       "      <td>0.145549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Text  Predicted_Class  Probability_Class_0  \\\n",
       "0  black people should die                4             0.071665   \n",
       "\n",
       "   Probability_Class_1  Probability_Class_2  Probability_Class_3  \\\n",
       "0             0.089742             0.091336             0.271571   \n",
       "\n",
       "   Probability_Class_4  Probability_Class_5  \n",
       "0             0.330138             0.145549  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"Text\": tweet_1,\n",
    "    \"Predicted_Class\": predicted_classes,\n",
    "}\n",
    "\n",
    "# Add probability columns for each class\n",
    "num_classes = model.config.num_labels\n",
    "for class_idx in range(num_classes):\n",
    "    data[f\"Probability_Class_{class_idx}\"] = probabilities[:, class_idx]\n",
    "\n",
    "df_1 = pd.DataFrame(data)\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa7dfa54-8ede-44d7-ac7f-6b9ff78cf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_response=df_1['Predicted_Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "558a9472-d904-4c21-bb4b-bfea922d5e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Text                 1 non-null      object \n",
      " 1   Predicted_Class      1 non-null      int64  \n",
      " 2   Probability_Class_0  1 non-null      float32\n",
      " 3   Probability_Class_1  1 non-null      float32\n",
      " 4   Probability_Class_2  1 non-null      float32\n",
      " 5   Probability_Class_3  1 non-null      float32\n",
      " 6   Probability_Class_4  1 non-null      float32\n",
      " 7   Probability_Class_5  1 non-null      float32\n",
      "dtypes: float32(6), int64(1), object(1)\n",
      "memory usage: 168.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb625005-3520-474a-9afb-fd2c9ca00460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Transphobia\n",
       "Name: Predicted_Class, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping_dict = {'Probability_Class_0': 'Racist tweet', 'Probability_Class_1': 'Religious Hate', 'Probability_Class_2': 'Xenophobia','Probability_Class_3': 'Misogyny','Probability_Class_4': 'Transphobia','Probability_Class_5': 'Homophobia'}\n",
    "\n",
    "mapping_dict = {0: 'Racist tweet', 1: 'Religious Hate', 2: 'Xenophobia',3: 'Misogyny',4: 'Transphobia',5: 'Homophobia'}\n",
    "\n",
    "y_response=df_1['Predicted_Class'].map(mapping_dict)\n",
    "y_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae736bf4-2dfe-4f10-84a6-ab3ba86d5885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daa905-d4dd-424e-b31b-d146f1eeb6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe0684-26b7-45c9-8f82-4804415df174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b8579-4150-4d0a-b619-9a6815b35a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eff37d-138e-48b0-81f9-abbe77cf5dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6935a8-d4d8-4e0a-bad8-714a9ae65265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d52a7-e96c-4f89-8548-90a48983ad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01e80c-48b8-4ec0-ad23-07a52fd6af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('prajjwal1/bert-mini')\n",
    "model = TFBertForSequenceClassification(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8700dd-139b-4a37-a8e1-7663a28ff139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed1e85-cae8-4f8c-a5e8-2e83a18a3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights-bert.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08f62f-2d45-4667-afb4-20f75dfaab21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09d039c-c815-4346-8332-68ab7b88f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y_true, y_pred, threshold=0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "# Apply the threshold to the predicted values\n",
    "    y_pred = tf.cast(y_pred > threshold, dtype=tf.float32)\n",
    "\n",
    " # Calculate the Hamming Loss\\\n",
    "    epsilon = 1e-7  # Small epsilon value\\n\",\n",
    "    hamming_loss = 1.0 - tf.reduce_mean(tf.reduce_sum(y_true * y_pred, axis=1) / (tf.reduce_sum(y_true + y_pred - y_true * y_pred, axis=1) + epsilon))\n",
    "\n",
    "    return hamming_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6856e753-936a-41e8-b855-75540e9a55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1958700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 32)          4832      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, None, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2017338 (7.70 MB)\n",
      "Trainable params: 2017338 (7.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "232/232 [==============================] - 25s 95ms/step - loss: 0.5233 - accuracy: 0.3479 - val_loss: 0.5178 - val_accuracy: 0.3489\n",
      "Epoch 2/150\n",
      "232/232 [==============================] - 22s 97ms/step - loss: 0.5164 - accuracy: 0.3500 - val_loss: 0.5171 - val_accuracy: 0.3489\n",
      "Epoch 3/150\n",
      "232/232 [==============================] - 21s 93ms/step - loss: 0.5164 - accuracy: 0.3500 - val_loss: 0.5169 - val_accuracy: 0.3489\n",
      "Epoch 4/150\n",
      "  9/232 [>.............................] - ETA: 20s - loss: 0.5081 - accuracy: 0.3568"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Change the number of epochs and the batch size depending on the RAM Size\u001b[39;00m\n\u001b[1;32m     38\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_pad, y_test)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe accuracy evaluated on the test set is of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=embedding_size))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Students will be ending their code here\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Change the number of epochs and the batch size depending on the RAM Size\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    callbacks=[es], validation_split=0.3\n",
    ")\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68c130-0d8f-43e8-8c4f-7b8e4054e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e630f225-f3d7-40e8-9052-4a1599891cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 50)          1958700   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, None, 20)          4320      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 20)                2520      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1965666 (7.50 MB)\n",
      "Trainable params: 1965666 (7.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "232/232 [==============================] - 28s 86ms/step - loss: 0.5344 - accuracy: 0.3064 - val_loss: 0.5160 - val_accuracy: 0.3490\n",
      "Epoch 2/3\n",
      "232/232 [==============================] - 18s 77ms/step - loss: 0.5074 - accuracy: 0.3593 - val_loss: 0.4843 - val_accuracy: 0.3717\n",
      "Epoch 3/3\n",
      "232/232 [==============================] - 18s 77ms/step - loss: 0.4455 - accuracy: 0.4473 - val_loss: 0.4329 - val_accuracy: 0.4657\n",
      "442/442 [==============================] - 5s 11ms/step - loss: 0.4325 - accuracy: 0.4663\n",
      "The accuracy evaluated on the test set is of 46.635%\n",
      "Testing loss \t 43.24631094932556\n",
      "Testing accuracy  46.63458168506622\n"
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(6, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=3,\n",
    "        batch_size=128,\n",
    "        callbacks=[es], validation_split=0.3\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b415ee06-6515-4d9e-97fd-378903cea860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 7s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8290219"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786f65bc-cb6b-448f-b42f-0bbc26386a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18742.92"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0272686-7c82-4b74-91bb-f0cbc65a2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the image from a file\n",
    "image = Image.open(\"../data/Background_site.png\")  # Replace with the actual path to your image file\n",
    "\n",
    "# Display the image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c439fd4-3fe0-4bcc-9a5e-af914e7793c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
