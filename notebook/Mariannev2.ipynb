{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe2301",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hateXplain = pd.read_json(\"/home/mariannettrd/code/irismarechal1997/moder_ia/raw_data/230911_HateXplain.json\")\n",
    "\n",
    "hateXplain_inversed = hateXplain.transpose()\n",
    "\n",
    "hateXplain=hateXplain_inversed\n",
    "\n",
    "\n",
    "hateXplain[\"source\"]=\"230911_HateXplain\" # add source of the doc\n",
    "\n",
    "\n",
    "hateXplain = hateXplain[[\"post_tokens\", \"annotators\", \"source\"]].copy()\n",
    "\n",
    "hateXplain.reset_index(inplace=True, drop=True)\n",
    "\n",
    "hateXplain[\"offensive\"]=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e02ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hateXplain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56c156",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0,len(hateXplain)): \n",
    "#     hateXplain.iloc[i,-1]= hateXplain.iloc[i,1][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740b5df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hateXplain[\"offensive\"]=hateXplain[\"annotators\"].apply(lambda x:x[0][\"label\"]).apply(lambda x:0 if x ==\"normal\" else 1)\n",
    "hateXplain[\"offensive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916917cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hateXplain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f2fa5-85cc-4a0b-91d8-e6056d2cdfdb",
   "metadata": {},
   "source": [
    "## ML for multilabelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baf594-20c3-4a8a-9b35-d6fa68bd9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fa41f-65f8-41e9-80ce-09e38fa954cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/mariannettrd/code/irismarechal1997/moder_ia/data/data_classif.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6659c-c1e6-4189-8225-4fd46bb683e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24feca2b-f12a-4d2b-93bb-3415f0504e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"Unnamed: 0\", inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32727881-b4dc-4906-9379-c48719bd8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns =\"hate_speech_score\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda4907-efc2-42b5-80d0-51fa981c678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da9f21-8cfd-443a-9188-0751e49fe9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictrename = {'target_race': 'racism',\n",
    "              'target_religion': 'religion',\n",
    "              'target_origin': 'xenophobia',\n",
    "              'target_gender_women':'misogyny',\n",
    "              'target_gender_without_women':'transphobia',\n",
    "              'target_sexuality': 'homophobia',\n",
    "              'target_age': 'ageism',\n",
    "              'target_disability':'validism'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10d2fc-4798-4d5d-a2d8-b1fb237e5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'target_race': 'racism',\n",
    "              'target_religion': 'religion',\n",
    "              'target_origin': 'xenophobia',\n",
    "              'target_gender_women':'misogyny',\n",
    "              'target_gender_without_women':'transphobia',\n",
    "              'target_sexuality': 'homophobia',\n",
    "              'target_age': 'ageism',\n",
    "              'target_disability':'validism'}, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6be37-3c61-48e6-8061-d50cdf75c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb09dd5-b4cc-463e-a991-44492be9d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ['racism', 'religion', 'xenophobia', 'misogyny', 'transphobia', 'homophobia', 'ageism', 'validism']:\n",
    "    df[row] = df[row].replace({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d4b7f-d37b-4d51-b95e-369a996a210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655bcc3-ad87-45ca-9d5b-152782ea066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_table(data):\n",
    "    data = data.drop_duplicates() # Remove duplicates\n",
    "    data = data.dropna(subset=['offensive']) # Remove n.a. values in columns 'Label' => check column\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfaa82-c641-4525-a508-cb1ce7015963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_table(data):\n",
    "    data = data.drop_duplicates() # Remove duplicates\n",
    "    data = data.dropna(subset=['text']) # Remove n.a. values in columns 'Label' => check column\n",
    "    return data\n",
    "\n",
    "# Note: no need to Scale the features, Encode features, Perform cyclical engineering\n",
    "\n",
    "def cleaning_text(sentence: str) -> str:\n",
    "\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "\n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize\n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "\n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    #remove words\n",
    "    removed = [\"user\", \"rt\"]\n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in removed\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "\n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a2ec9-61d0-405a-9f06-08b69ecd9638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_processed = cleaning_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37837bf5-43ff-45cd-a96a-10f7471b7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed[\"text_processed\"] = data_processed[\"text\"].apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55cb3f-df11-411c-8e62-afa641da45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e2215-0b2d-4d65-9f89-326fee6346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd0f32-ba58-4866-858e-a7d277817fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "\n",
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName, test_labels, predictions):\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "    print(\"Accuracy: {:.4f}\\nHamming Loss: {:.4f}\\nPrecision:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nRecall:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nF1-measure:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\"\\\n",
    "          .format(accuracy, hamLoss, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))\n",
    "    ModelsPerformance[modelName] = micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd172d7-7d27-4799-b5a8-8996614abcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_processed[\"text\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0cab6-3e75-4d73-aa01-4195385181e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline vectorizer + LinearSVC\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "svmPreds = pipeline.predict(X_test)\n",
    "metricsReport(\"SVC Sq. Hinge Loss\", y_test, svmPreds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f591350-e707-4ac0-9b77-2ccf9daa2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    OneVsRestClassifier(MultinomialNB()))\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "svmPreds = pipeline.predict(X_test)\n",
    "metricsReport(\"Multinomial NB\", y_test, svmPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922366f4-9abb-4b7b-90a8-3f6cdd2015ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "#import keras\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651d1f8-d55d-45fa-ab08-1dfb1d46fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing data\n",
    "X = data_processed[\"text\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(8, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        callbacks=[es]\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bc401-dd0d-415f-8926-29d2308e9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "# create the model\n",
    "max_review_length = 600\n",
    "\n",
    "embedding_vector_length = 32\n",
    "cnn_model = Sequential()\n",
    "\n",
    "\n",
    "cnn_model.add(Embedding(input_dim=vocab_size+1, output_dim=embedding_size))   \n",
    "cnn_model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(layers.LSTM(100))\n",
    "cnn_model.add(layers.Dense(8, activation='sigmoid'))\n",
    "\n",
    "# Students will be ending their code here\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(cnn_model.summary())\n",
    "\n",
    "# Change the number of epochs and the batch size depending on the RAM Size\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "history_c =model.fit(X_train_pad, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        callbacks=[es]\n",
    "        )\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61042bbd-d028-4060-bc9f-9e5fd356db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(X_train_pad)\n",
    "test_predictions = model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282b393-9e52-43c4-8a3a-56add8962dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3ebb5-cca8-4fa5-baf3-00b7a85a63ae",
   "metadata": {},
   "source": [
    "## One model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0bdf66-55cd-4ccf-a156-bd91e056a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d31c1-31b5-4b92-b305-f6147f8d12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/labelling_dataset_v1.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f0519-3947-47e5-bcb7-b2b1a68d1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d90723-dcdb-4a20-b9d1-8046dd589baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = pd.read_csv(\"/home/mariannettrd/code/irismarechal1997/moder_ia/data/labelling_dataset_v1.csv\")\n",
    "labelled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a774a-4c0e-404a-8607-597cf0397548",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(56515, dtype=np.int8 )\n",
    "list_1 = label.tolist()\n",
    "labelled_data.insert(1,\"non-offensive\",list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b1b18-3a77-4c7f-88b8-e5338b09a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb2634-86e1-4956-8097-0fae02d7fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df=pd.DataFrame([\n",
    "    {\"racism\":len(labelled_data[labelled_data[\"racism\"]==1]),\n",
    "       \"religion\":len(labelled_data[labelled_data[\"religion\"]==1]),\n",
    "       \"xenophobia\":len(labelled_data[labelled_data[\"xenophobia\"]==1]),\n",
    "       \"misogyny\":len(labelled_data[labelled_data[\"misogyny\"]==1]),\n",
    "     \"non-offensive\":len(labelled_data[labelled_data[\"non-offensive\"]==1]),\n",
    "       \"transphobia\":len(labelled_data[labelled_data[\"transphobia\"]==1]),\n",
    "       \"validism\":len(labelled_data[labelled_data[\"homophobia\"]==1])}])\n",
    "val_df, val_df.iloc[0].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d106bcc-d214-44f2-a5cc-06c6f312c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/mariannettrd/code/irismarechal1997/moder_ia/data/processed_dataset_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627d0b0-0a52-49fd-b25c-e09d4e72571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ea943-ac21-48b9-a9ec-2c01bd57c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select number of tweets\n",
    "number_rows = 10_000\n",
    "\n",
    "#filter on non-offensive\n",
    "df2 = df[df[\"offensive\"]==0]\n",
    "sample_df=df2.sample(number_rows)\n",
    "\n",
    "#select right columns to keep\n",
    "selected_features = [\"text\",\"offensive\", \"text_processed\"]\n",
    "new_df = sample_df[selected_features].copy()\n",
    "label = np.zeros(number_rows, dtype=np.int8 )\n",
    "list_2 = label.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43ca09-6539-4961-aacb-92807eea14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d49085-2ec7-458b-a205-0f4d22f37d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data.head()\n",
    "labelled_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c77715-1a69-48ed-8960-dbca1150fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(number_rows, dtype=np.int8 )\n",
    "list_2 = label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae961f-9215-4efe-a412-84be2301c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3cca5-61c0-424a-b9f0-43aee3bd083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame based on sample_df\n",
    "new_df = sample_df[selected_features].copy()\n",
    "\n",
    "# Define the columns you want to insert\n",
    "# columns = ['non-offensive', 'racism', 'religion', 'xenophobia', 'misogyny', 'transphobia', 'homophobia']\n",
    "columns= ['homophobia', 'transphobia', 'misogyny', 'xenophobia', 'religion', 'racism', 'non-offensive']\n",
    "# Iterate through the columns and insert them into new_df\n",
    "for col in columns:\n",
    "    new_df.insert(1, col, list_2)\n",
    "\n",
    "new_df[\"non-offensive\"]=1\n",
    "new_df.drop(columns=\"offensive\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7e67c-d9d9-412c-b7dc-436d47578a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89da6ae-2af1-432a-b0de-868b6ad44a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8908174-2b46-4b66-8337-2fdf04317365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set= pd.concat([labelled_data, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6a126-5229-45ab-834f-4e7a7e84ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e516e-ffa7-4576-9c55-73138b85badd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into training and testing data\n",
    "X = data_set[\"text\"]\n",
    "y = data_set.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(7, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=150,\n",
    "        batch_size=128,\n",
    "        callbacks=[es], validation_split=0.3\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb502b-3876-4544-8fc6-b972ebd65974",
   "metadata": {},
   "source": [
    "3. Model Iris avec Hamming loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ed0238-2444-4fc0-85ab-f193d4fa1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c88504-456e-4d8a-9221-832354be76db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          2932850   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, None, 20)          4320      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 20)                2520      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 147       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2939837 (11.21 MB)\n",
      "Trainable params: 2939837 (11.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "273/273 [==============================] - 53s 162ms/step - loss: 0.4854 - accuracy: 0.2573 - val_loss: 0.4581 - val_accuracy: 0.2895\n",
      "Epoch 2/50\n",
      "273/273 [==============================] - 43s 159ms/step - loss: 0.4280 - accuracy: 0.3603 - val_loss: 0.3935 - val_accuracy: 0.4302\n",
      "Epoch 3/50\n",
      "273/273 [==============================] - 50s 183ms/step - loss: 0.3707 - accuracy: 0.4733 - val_loss: 0.3556 - val_accuracy: 0.5022\n",
      "Epoch 4/50\n",
      "273/273 [==============================] - 45s 164ms/step - loss: 0.3371 - accuracy: 0.5234 - val_loss: 0.3386 - val_accuracy: 0.5133\n",
      "Epoch 5/50\n",
      "273/273 [==============================] - 46s 169ms/step - loss: 0.3160 - accuracy: 0.5549 - val_loss: 0.3307 - val_accuracy: 0.5309\n",
      "Epoch 6/50\n",
      "273/273 [==============================] - 45s 164ms/step - loss: 0.3008 - accuracy: 0.5883 - val_loss: 0.3291 - val_accuracy: 0.5477\n",
      "Epoch 7/50\n",
      "273/273 [==============================] - 45s 163ms/step - loss: 0.2883 - accuracy: 0.6162 - val_loss: 0.3251 - val_accuracy: 0.5529\n",
      "Epoch 8/50\n",
      "273/273 [==============================] - 45s 165ms/step - loss: 0.2771 - accuracy: 0.6339 - val_loss: 0.3288 - val_accuracy: 0.5455\n",
      "Epoch 9/50\n",
      "273/273 [==============================] - 45s 165ms/step - loss: 0.2668 - accuracy: 0.6480 - val_loss: 0.3198 - val_accuracy: 0.5652\n",
      "Epoch 10/50\n",
      "273/273 [==============================] - 46s 167ms/step - loss: 0.2582 - accuracy: 0.6587 - val_loss: 0.3168 - val_accuracy: 0.5661\n",
      "Epoch 11/50\n",
      "273/273 [==============================] - 45s 165ms/step - loss: 0.2492 - accuracy: 0.6719 - val_loss: 0.3211 - val_accuracy: 0.5690\n",
      "Epoch 12/50\n",
      "273/273 [==============================] - 50s 184ms/step - loss: 0.2422 - accuracy: 0.6820 - val_loss: 0.3193 - val_accuracy: 0.5714\n",
      "Epoch 13/50\n",
      "273/273 [==============================] - 55s 200ms/step - loss: 0.2354 - accuracy: 0.6914 - val_loss: 0.3218 - val_accuracy: 0.5712\n",
      "Epoch 14/50\n",
      "273/273 [==============================] - 47s 174ms/step - loss: 0.2289 - accuracy: 0.7008 - val_loss: 0.3315 - val_accuracy: 0.5706\n",
      "Epoch 15/50\n",
      "273/273 [==============================] - 50s 182ms/step - loss: 0.2227 - accuracy: 0.7089 - val_loss: 0.3309 - val_accuracy: 0.5595\n",
      "Epoch 16/50\n",
      "273/273 [==============================] - 49s 179ms/step - loss: 0.2174 - accuracy: 0.7163 - val_loss: 0.3351 - val_accuracy: 0.5579\n",
      "Epoch 17/50\n",
      "273/273 [==============================] - 50s 184ms/step - loss: 0.2123 - accuracy: 0.7244 - val_loss: 0.3411 - val_accuracy: 0.5632\n",
      "Epoch 18/50\n",
      "273/273 [==============================] - 48s 177ms/step - loss: 0.2076 - accuracy: 0.7265 - val_loss: 0.3427 - val_accuracy: 0.5634\n",
      "Epoch 19/50\n",
      "273/273 [==============================] - 56s 205ms/step - loss: 0.2036 - accuracy: 0.7331 - val_loss: 0.3423 - val_accuracy: 0.5625\n",
      "Epoch 20/50\n",
      "273/273 [==============================] - 63s 231ms/step - loss: 0.1998 - accuracy: 0.7372 - val_loss: 0.3461 - val_accuracy: 0.5580\n",
      "520/520 [==============================] - 13s 20ms/step - loss: 0.3155 - accuracy: 0.5693\n",
      "The accuracy evaluated on the test set is of 56.931%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labelled_data = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "label = np.zeros(56515, dtype=np.int8 )\n",
    "list_1 = label.tolist()\n",
    "labelled_data.insert(1,\"non-offensive\",list_1)\n",
    "\n",
    "df = pd.read_csv(\"../data/\"+\"processed_dataset_v1.csv\")\n",
    "\n",
    "\n",
    "#select number of tweets\n",
    "number_rows = 10_000\n",
    "\n",
    "#filter on non-offensive\n",
    "df2 = df[df[\"offensive\"]==0]\n",
    "sample_df=df2.sample(number_rows)\n",
    "\n",
    "#select right columns to keep\n",
    "selected_features = [\"text\",\"offensive\", \"text_processed\"]\n",
    "new_df = sample_df[selected_features].copy()\n",
    "label = np.zeros(number_rows, dtype=np.int8 )\n",
    "list_2 = label.tolist()\n",
    "\n",
    "# Define the columns you want to insert\n",
    "columns= ['homophobia', 'transphobia', 'misogyny', 'xenophobia', 'religion', 'racism', 'non-offensive']\n",
    "\n",
    "# Iterate through the columns and insert them into new_df\n",
    "for col in columns:\n",
    "    new_df.insert(1, col, list_2)\n",
    "new_df[\"non-offensive\"]=1\n",
    "new_df.drop(columns=\"offensive\", inplace = True)\n",
    "\n",
    "data_set= pd.concat([labelled_data, new_df])\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data_set[\"text\"]\n",
    "y = data_set.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(7, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[es], validation_split=0.3\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72798027-9d4d-42ab-a2a5-957550566da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=full_model_classif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96a8f90-e577-44d0-bc48-0461a55f4337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 12s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0555336 , 0.32921416, 0.00551936, ..., 0.47810352, 0.07550936,\n",
       "        0.01757518],\n",
       "       [0.02533962, 0.01991368, 0.02846691, ..., 0.16541786, 0.42184418,\n",
       "        0.80412364],\n",
       "       [0.05737745, 0.07361521, 0.00563814, ..., 0.5634848 , 0.14435072,\n",
       "        0.06098539],\n",
       "       ...,\n",
       "       [0.9931581 , 0.00838253, 0.00795216, ..., 0.00990019, 0.0029719 ,\n",
       "        0.01034239],\n",
       "       [0.92378044, 0.00950552, 0.06980921, ..., 0.00542451, 0.00521516,\n",
       "        0.02108896],\n",
       "       [0.0600859 , 0.04420226, 0.33671063, ..., 0.00941777, 0.06304487,\n",
       "        0.16687848]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_pad)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "520822c8-6687-4185-9a29-8fe1d916c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true=y_test\n",
    "y_pred=y_pred\n",
    "\n",
    "def hamming_loss(y_true, y_pred, threshold=0.5):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Apply the threshold to the predicted values\n",
    "    y_pred = tf.cast(y_pred > threshold, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate the Hamming Loss\n",
    "    epsilon = 1e-7  # Small epsilon value\n",
    "    hamming_loss = 1.0 - tf.reduce_mean(tf.reduce_sum(y_true * y_pred, axis=1) / (tf.reduce_sum(y_true + y_pred - y_true * y_pred, axis=1) + epsilon))\n",
    "    \n",
    "    return hamming_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18ac5675-d0ad-4dbd-80ae-9ffe3079f4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48068637"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "h1 = hamming_loss(y_true, y_pred)\n",
    "h1\n",
    "\n",
    "\n",
    "#metric = metrics.HammingLoss(mode='multilabel', threshold=0.6)\n",
    "\n",
    "\n",
    "#metric.update_state(y_true,y_pred)\n",
    "#metric.result().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f7d4f-da84-499f-8cd9-8846b8df6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e8514-06db-4c43-825b-71619fc2c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_classif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b6be51-0d07-437c-a0b6-4f0a17ca36b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 50)          1958700   \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, None, 20)          4320      \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 20)                2520      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1965666 (7.50 MB)\n",
      "Trainable params: 1965666 (7.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "232/232 [==============================] - 27s 83ms/step - loss: 0.5297 - accuracy: 0.3490 - val_loss: 0.5163 - val_accuracy: 0.3509\n",
      "Epoch 2/150\n",
      "232/232 [==============================] - 18s 76ms/step - loss: 0.5017 - accuracy: 0.3828 - val_loss: 0.4650 - val_accuracy: 0.4303\n",
      "Epoch 3/150\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 0.4371 - accuracy: 0.4721 - val_loss: 0.4291 - val_accuracy: 0.4693\n",
      "Epoch 4/150\n",
      "232/232 [==============================] - 18s 76ms/step - loss: 0.4166 - accuracy: 0.4887 - val_loss: 0.4186 - val_accuracy: 0.4843\n",
      "Epoch 5/150\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 0.4000 - accuracy: 0.4921 - val_loss: 0.4055 - val_accuracy: 0.4840\n",
      "Epoch 6/150\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 0.3764 - accuracy: 0.5302 - val_loss: 0.3839 - val_accuracy: 0.5274\n",
      "Epoch 7/150\n",
      "232/232 [==============================] - 21s 90ms/step - loss: 0.3510 - accuracy: 0.5845 - val_loss: 0.3778 - val_accuracy: 0.5344\n",
      "Epoch 8/150\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 0.3324 - accuracy: 0.6179 - val_loss: 0.3600 - val_accuracy: 0.5628\n",
      "Epoch 9/150\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 0.3176 - accuracy: 0.6338 - val_loss: 0.3592 - val_accuracy: 0.5630\n",
      "Epoch 10/150\n",
      "232/232 [==============================] - 20s 85ms/step - loss: 0.3065 - accuracy: 0.6443 - val_loss: 0.3583 - val_accuracy: 0.5748\n",
      "Epoch 11/150\n",
      "232/232 [==============================] - 19s 82ms/step - loss: 0.2968 - accuracy: 0.6568 - val_loss: 0.3569 - val_accuracy: 0.5671\n",
      "Epoch 12/150\n",
      "232/232 [==============================] - 19s 83ms/step - loss: 0.2871 - accuracy: 0.6668 - val_loss: 0.3534 - val_accuracy: 0.5764\n",
      "Epoch 13/150\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 0.2792 - accuracy: 0.6755 - val_loss: 0.3567 - val_accuracy: 0.5776\n",
      "Epoch 14/150\n",
      "232/232 [==============================] - 19s 82ms/step - loss: 0.2717 - accuracy: 0.6842 - val_loss: 0.3590 - val_accuracy: 0.5812\n",
      "Epoch 15/150\n",
      "232/232 [==============================] - 19s 81ms/step - loss: 0.2646 - accuracy: 0.6930 - val_loss: 0.3605 - val_accuracy: 0.5700\n",
      "Epoch 16/150\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 0.2579 - accuracy: 0.7002 - val_loss: 0.3687 - val_accuracy: 0.5876\n",
      "442/442 [==============================] - 5s 12ms/step - loss: 0.3504 - accuracy: 0.5846\n",
      "The accuracy evaluated on the test set is of 58.461%\n",
      "Testing loss \t 35.038867592811584\n",
      "Testing accuracy  58.46132040023804\n"
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, # size of the input, impacting the number of weights in the linear combinations of the neurons of the first layer\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.GRU(20, return_sequences=True, activation=\"tanh\"))\n",
    "model.add(layers.GRU(20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(6, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "        epochs=150,\n",
    "        batch_size=128,\n",
    "        callbacks=[es], validation_split=0.3\n",
    "        )\n",
    "\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5ca3593-27c2-4834-86f4-53c2c61fd6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 7s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4625392"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h2 = hamming_loss(y_true, y_pred)\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94caab34-8acf-44a8-b77e-39c6b624070b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/labelling_dataset_v1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabelling_dataset_v1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Split into training and testing data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m data_processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/moder_ia/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/labelling_dataset_v1.csv'"
     ]
    }
   ],
   "source": [
    "data_processed = pd.read_csv(\"../data/\"+\"labelling_dataset_v1.csv\")\n",
    "# Split into training and testing data\n",
    "X = data_processed[\"text_processed\"]\n",
    "y = data_processed.drop(labels=[\"text_processed\", \"text\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "### Let's tokenize the vocabulary\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 50\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=embedding_size))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Students will be ending their code here\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Change the number of epochs and the batch size depending on the RAM Size\n",
    "\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    callbacks=[es], validation_split=0.3\n",
    ")\n",
    "res = model.evaluate(X_test_pad, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print('Testing loss \\t', res[0]*100)\n",
    "print('Testing accuracy ', res[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1290d-eda9-4c22-97a2-071a813a94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred=model.predict(X_test_pad)\n",
    "\n",
    "h3 = hamming_loss(y_true, y_pred)\n",
    "h3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
